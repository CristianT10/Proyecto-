{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.autocasion.com\",\n",
    "    \"Accept-Language\": \"es-ES,es;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "ESTADO_FILE = \"estado_extraccion.json\"\n",
    "CARPETA_BLOQUES = \"bloques_detalles\"\n",
    "os.makedirs(CARPETA_BLOQUES, exist_ok=True)\n",
    "\n",
    "TAMANO_BLOQUE = 1000\n",
    "\n",
    "\n",
    "def cargar_estado():\n",
    "    if os.path.exists(ESTADO_FILE):\n",
    "        with open(ESTADO_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def guardar_estado(estado):\n",
    "    with open(ESTADO_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(estado, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extraer_detalles(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        ul_ficha = soup.find(\"ul\", class_=\"datos-basicos-ficha\")\n",
    "        detalles = [li.get_text(strip=True) for li in ul_ficha.find_all(\"li\")] if ul_ficha else []\n",
    "\n",
    "        precios = []\n",
    "        ul_precios = soup.find(\"ul\", class_=\"tabla-precio\")\n",
    "        if ul_precios:\n",
    "            for li in ul_precios.find_all(\"li\"):\n",
    "                spans = li.find_all(\"span\")\n",
    "                if spans:\n",
    "                    clave = spans[0].get_text(strip=True)\n",
    "                    valor = spans[1].get_text(strip=True) if len(spans) > 1 else \"\"\n",
    "                    precios.append(f\"{clave} {valor}\")\n",
    "\n",
    "        return {\n",
    "            \"detalles_ficha\": detalles,\n",
    "            \"precios\": precios\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extrayendo {url}: {e}\")\n",
    "        return {\n",
    "            \"detalles_ficha\": [],\n",
    "            \"precios\": []\n",
    "        }\n",
    "\n",
    "\n",
    "def procesar_bloque(indice, urls):\n",
    "    print(f\"Procesando bloque {indice + 1} con {len(urls)} URLs...\")\n",
    "    resultados = []\n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"  [{i+1}/{len(urls)}] {url}\")\n",
    "        datos = extraer_detalles(url)\n",
    "        resultados.append({\n",
    "                                \"url\": url,\n",
    "                                \"detalles_ficha\": datos.get(\"detalles_ficha\", []),\n",
    "                                \"precios\": datos.get(\"precios\", [])\n",
    "})\n",
    "\n",
    "        time.sleep(0.2)\n",
    "    archivo = os.path.join(CARPETA_BLOQUES, f\"bloque_{indice + 1}.json\")\n",
    "    with open(archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(resultados, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Bloque {indice + 1} guardado en {archivo}\")\n",
    "\n",
    "def dividir_en_bloques(lista_urls, tamano):\n",
    "    return [lista_urls[i:i + tamano] for i in range(0, len(lista_urls), tamano)]\n",
    "\n",
    "def main():\n",
    "    print(\"Leyendo CSV de URLs...\")\n",
    "    df = pd.read_csv(\"anuncios_unificados1.csv\")\n",
    "    if \"url\" not in df.columns:\n",
    "        raise ValueError(\"El CSV no contiene la columna 'url'.\")\n",
    "\n",
    "    urls = df[\"url\"].dropna().unique().tolist()\n",
    "    print(f\"Total URLs únicas: {len(urls)}\")\n",
    "\n",
    "    bloques_urls = dividir_en_bloques(urls, TAMANO_BLOQUE)\n",
    "    print(f\"Dividido en {len(bloques_urls)} bloques de tamaño {TAMANO_BLOQUE}\")\n",
    "\n",
    "    estado = cargar_estado()\n",
    "\n",
    "    for i, bloque in enumerate(bloques_urls):\n",
    "        if estado.get(str(i)) is True:\n",
    "            print(f\"Bloque {i + 1} ya procesado. Saltando.\")\n",
    "            continue\n",
    "        procesar_bloque(i, bloque)\n",
    "        estado[str(i)] = True\n",
    "        guardar_estado(estado)\n",
    "        print(f\"Estado actualizado. Bloque {i + 1} completado.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
