{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102d9fc4",
   "metadata": {},
   "source": [
    "# Depuracion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14aaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d775122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datos_limpios.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376941d8",
   "metadata": {},
   "source": [
    "* Imputamos NaNs con la moda de las columnas categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064b7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['combustible', 'transmision', 'tipo_carroceria']\n",
    "\n",
    "imputers = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[[col]] = imputer.fit_transform(df[[col]])\n",
    "    imputers[col] = imputer\n",
    "    \n",
    "    with open(f'encoders/imputer_{col}.pkl', 'wb') as f:\n",
    "        pickle.dump(imputer, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657928c",
   "metadata": {},
   "source": [
    "* Imputamos NaNs de columnas numericas con KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6def63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_to_impute = ['mes_matriculacion', 'potencia_cv', 'puertas', 'asientos']\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "df[num_cols_to_impute] = knn_imputer.fit_transform(df[num_cols_to_impute])\n",
    "\n",
    "with open('encoders/knn_imputer_num_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_imputer, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd326d33",
   "metadata": {},
   "source": [
    "* Cambiamos el tipo de dato a las columnas seleccionadas despues de imputar NaNs ya que con el KNNImputer te deja los numeros en decimales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4b31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_int = ['mes_matriculacion', 'potencia_cv', 'puertas', 'asientos']\n",
    "\n",
    "for col in cols_int:\n",
    "    df[col] = df[col].round().astype('Int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68855db",
   "metadata": {},
   "source": [
    "* Hacemos target encoding de las columnas \"marca\" y \"modelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ec412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(train_df, col, target):\n",
    "    encoding_map = train_df.groupby(col)[target].mean()\n",
    "    \n",
    "    train_df[col + '_te'] = train_df[col].map(encoding_map)\n",
    "    \n",
    "    with open(f'encoders/target_encoding_{col}.pkl', 'wb') as f:\n",
    "        pickle.dump(encoding_map, f)\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "df = target_encode(df, 'marca', 'precio_contado')\n",
    "df = target_encode(df, 'modelo', 'precio_contado')\n",
    "\n",
    "df = df.drop(['marca', 'modelo'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f89573",
   "metadata": {},
   "source": [
    "* Hacemos encoding binario de la columna \"transmision\" ya que solo contiene dos valores diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "transmision_map = {'Manual': 0, 'Automático': 1}\n",
    "\n",
    "df['transmision_bin'] = df['transmision'].map(transmision_map)\n",
    "\n",
    "df.drop(columns=['transmision'], inplace=True)\n",
    "\n",
    "with open('encoders/bin_encoder_transmision.pkl', 'wb') as f:\n",
    "    pickle.dump(transmision_map, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f0f20",
   "metadata": {},
   "source": [
    "* Hacemos One Hot Encoding para la columna \"combustible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7193c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ohe_combustible = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "combustible_encoded = ohe_combustible.fit_transform(df[['combustible']])\n",
    "\n",
    "combustible_cols = [f\"combustible_{cat}\" for cat in ohe_combustible.categories_[0]]\n",
    "df_combustible = pd.DataFrame(combustible_encoded, columns=combustible_cols, index=df.index)\n",
    "\n",
    "df = pd.concat([df, df_combustible], axis=1)\n",
    "df.drop(columns=['combustible'], inplace=True)\n",
    "\n",
    "with open('encoders/ohe_combustible.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe_combustible, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46290785",
   "metadata": {},
   "source": [
    "* One Hot Encoding de la columna \"tipo_carroceria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b272ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_carroceria = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "carroceria_encoded = ohe_carroceria.fit_transform(df[['tipo_carroceria']])\n",
    "\n",
    "carroceria_cols = [f\"tipo_carroceria_{cat}\" for cat in ohe_carroceria.categories_[0]]\n",
    "df_carroceria = pd.DataFrame(carroceria_encoded, columns=carroceria_cols, index=df.index)\n",
    "\n",
    "df = pd.concat([df, df_carroceria], axis=1)\n",
    "df.drop(columns=['tipo_carroceria'], inplace=True)\n",
    "\n",
    "with open('encoders/ohe_tipo_carroceria.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe_carroceria, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cce84",
   "metadata": {},
   "source": [
    "* Eliminamos columnas que no necesitamos para el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e176fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\n",
    "    'id_extraccion',\n",
    "    'timestamp_extraccion',\n",
    "    'ubicacion'\n",
    "], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdacac",
   "metadata": {},
   "source": [
    "* Separamos la variable objetivo (\"y\") del resto de las variables predictoras (\"X\"). En \"X\" se almacenan todas las columnas excepto el target, y en \"y\" únicamente la columna \"precio_contado\". (Alfinal, hemos aplicado el escalado solo a las columnas seleccionadas, ya que trabajaremos con esas columnas pra entrenar el modelo siendo las 10 columnas con mas importancia)\n",
    "* Aplicamos \"StandardScaler\" a \"X\" para escalar todas las variables predictoras, lo que garantiza que tengan media 0 y desviación estándar 1, mejorando el rendimiento de muchos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e9a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_X = [\n",
    "    'modelo_te', 'kilometraje', 'potencia_cv', 'año_matriculacion',\n",
    "    'combustible_Diésel', 'tipo_carroceria_Deportivo', 'transmision_bin',\n",
    "    'combustible_Eléctrico', 'financiacion_disponible',\n",
    "    'combustible_Híbrido Enchufable']\n",
    "\n",
    "X = df[columnas_X]\n",
    "y = df['precio_contado']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "with open(\"encoders/standard_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e93e5",
   "metadata": {},
   "source": [
    "* Creamos un nuevo DataFrame (\"df_final\") que combina las variables escaladas con la variable objetivo (\"precio_contado\") y lo guardamos en un .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e96c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = X_scaled_df.copy()\n",
    "df_final['precio_contado'] = y\n",
    "df_final.to_csv('datos_limpios_modelo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f7d4d",
   "metadata": {},
   "source": [
    "* Dividimos los datos en conjuntos de entrenamiento y prueba usando \"train_test_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6ed547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (70739, 30), X_test: (17685, 30)\n",
      "y_train: (70739,), y_test: (17685,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
